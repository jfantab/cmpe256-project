%% \documentclass[10pt]{article}
%% \documentclass[10pt,technote]{IEEEtran}
\documentclass[conference]{IEEEtran}

\usepackage[skip=7pt plus1pt, indent=0pt]{parskip}
\usepackage{hyperref}
\usepackage[margin=1.0in]{geometry}
\usepackage{cite}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\begin{document}

\title{\textsc{CMPE-256}: Million Songs Dataset}
\date{}

\author{
\IEEEauthorblockN{Carlos Hernandez}
\IEEEauthorblockA{
\textit{Computer Engineering Dept.} \\
\textit{San Jose State University} \\
{\small \texttt{carlos.hernandez@sjsu.edu}}}
\and 
\IEEEauthorblockN{Hardy Leung}
\IEEEauthorblockA{
\textit{Computer Engineering Dept.} \\
\textit{San Jose State University} \\
{\small \texttt{kwok-shing.leung@sjsu.edu}}}
\and 
\IEEEauthorblockN{John Lu}
\IEEEauthorblockA{
\textit{Computer Engineering Dept.} \\
\textit{San Jose State University} \\
{\small \texttt{john.lu@sjsu.edu}}}
}

\maketitle

\begin{abstract}
We propose to investigate the Million Songs Dataset (MSD) \cite{bertin2011million},
a collection of user data and metadata for popular contemporary songs. In particular, we will look at the user taste subset of datasets which consists of users, songs, and their respective play counts. The training data is approximately 500 MB (compressed) and has 1 million users, 380,000 unique songs, and 48 million user-song play count triplets. The test data has 100K users, and the validation data has 10K users. Note that users don't rate the songs; instead, we are given how many times a user played a certain song.
\end{abstract}

\begin{IEEEkeywords}
recommender system, implicit feedback
\end{IEEEkeywords}

\section{Dataset}

We propose to investigate the Million Songs Dataset (MSD) \cite{bertin2011million},
a collection of user data and metadata for popular contemporary songs. In particular, we will look at the user taste subset of datasets which consists of users, songs, and their respective play counts. The training data is approximately 500 MB (compressed) and has 1 million users, 380,000 unique songs, and 48 million user-song play count triplets. The test data has 100K users, and the validation data has 10K users. Note that users don't rate the songs; instead, we are given how many times a user played a certain song.

% Link to training data: \texttt{http://millionsongdataset.com/tasteprofile/}

% Link to testing/evaluation data: \texttt{http://millionsongdataset.com/challenge/\#data1}

We will be using the truncated mAP (mean average precision) as the evaluation metric. For users in only the validation test sets, half of the songs in each player's playlist are kept hidden, and the recommendations for each user will be compared to the hidden half. 

\section{Metadata}

We intend to use additional metadata provided as part of the Million Songs Dataset. We believe the following datasets would be helpful to our research.

\subsection{Last.fm dataset \cite{bertin2011million}}

{\small \texttt{http://millionsongdataset.com/lastfm/}}

This dataset contains annotations for 943,347 tracks. An annotation may include tags (such as
\textsc{pop}, \textsc{classic rock}, etc), as well as a list of similar songs. We will be mindful of the uneven quality of the annotation, as some songs were heavily annotated while others are clearly not.

\subsection{Tagtraum Genre Labels}

{\small \texttt{https://www.tagtraum.com/msd\_genre\_datasets.html}}

This dataset, curated by Henrik Schreiber \cite{schreiber2015improving}
contains genre annotations for up to 280,831 tracks.
The annotations are of the form \textsc{rock}, \textsc{blues}, etc.

\subsection{The MusiXMatch Dataset}

{\small \texttt{http://millionsongdataset.com/musixmatch/}}

This dataset contains the lyrics of 237,662 tracks in the form of stemmed bag-of-words. There are, however, a variety of factors that limit the number of tracks with lyrical information, including copyright considerations, songs without vocal tracks, etc. It is not clear whether the high degree of missing features would hamper the value of this dataset for our project, but we could assume popular songs should have the lyrical information available.

\subsection{MSD Allmusic Genre Dataset (MAGD)}

{\small \texttt{http://www.ifs.tuwien.ac.at/mir/msd/}}

This dataset provides another single-word genre annotation for up to 422,714 tracks.

\subsection{MSD Acoustic and Spectrogram Dataset}

{\small \texttt{http://www.ifs.tuwien.ac.at/mir/msd/}}

This dataset is actually generated by the same group and in fact is the basis for the MAGD genre classification. In particular, an audio sample of each song is analyzed into a 2D power spectrum that reflects human loudness sensation \cite{lidy2005evaluation} of dimension 24 $\times$ 60 = 1440 features (24 bands and 60 modulation frequencies per band). The power spectrum is also further analyzed into 7 features per band, detailing mean, median, variance, skewness, kurtosis, min- and max-value per band. It would be up to us to experiment with this information to perform similarity analysis between songs.

Overall, the metadata that we will consider include: (a) similar tracks, (b) genre classification, (c) lyrics, and (d) spectral information. It would allow us to perform similarity analysis based on metadata.

\section{Proposed Methods}

We will consider applying the following methods, carefully curated based on our specific problem formulation. That said, we may not know how well each technique would work until we go further into the experiments.

\subsection{Hybrid Neighborhood-based Collaborative Filtering}

We will consider both item-based and user-based methods, with adjustments to both the similarity metrics as well as how they should be used as weights, including variants such as Jaccard similarity with an exponentiated normalization that can be tuned as hyperparameters, and another non-linear weighting factor similar to the work done by Aiolli
\cite{aiolli2013efficient}. For example, using Jaccard similarity (1 means having listen to the song at least once, and 0 means never), the weights between items i and j can be calculated as

\begin{eqnarray*}
w(i, j) = \left(\frac{|\textrm{common}(i, j)|}{(|\textrm{item}(i)|^\alpha \times |\textrm{item}(j)|^{(1-\alpha)}}\right)^\gamma
\end{eqnarray*}

where $\alpha$ and $\gamma$ are hyperparameters to be tuned. The datasets are significantly larger than what many Python recommender systems can handle (e.g. Surprise will fail, since it constructs either MxM or NxN similarity matrices). We will need custom C++ implementation to handle our datasets, which must take advantage of the sparsity of the matrix. For example, if we provide two separate views of the utility matrix, one sorted by U and then I, and one sorted by I and then U, we believe we can calculate the similarity scores with the optimal complexity.

\subsection{Matrix Factorization-based Collaborative Filtering for Implicit Feedback}

Unlike collaborative filtering with explicit feedback, implicit feedback such as the number of plays is best modeled as a measure of preference \cite{hu2008collaborative}. Songs that have not been played will be assigned a low preference, and as such, technically the utility matrix would be fully populated. This is different from MF with explicit feedback where only explicitly rated user-item pairs show up in the loss function. We will adopt a modified version of the MF formulation to address this problem to avoid long runtime, along the line of Hu et al.~\cite{johnson2014logistic}. We will also look into the use of logistic matrix factorization for implicit feedback a la Johnson [7] to better model the probabilistic nature of the preference.

Clustering based on Computing Embeddings from Utility Matrix
Barkan, Oren, and Koenigstein \cite{barkan2016item2vec} showed that the word2vec algorithm for NLP can be adapted to extract the latent embedding of songs based on the utility matrix alone, and demonstrated its superiority over standard MF formulation. We believe in the merit of their argument, and would like to consider implementing such an approach. However, we are mindful of the potential pitfall in applying an item-based approach with a similarity metric that measures the distance between embeddings, as a naive implementation would perform O(NxN) comparisons at a runtime complexity of O(NxNxd), where N is close to 1 million, and d (the dimension of the embeddings) is between 5 and 100. This is clearly impractical, and necessitates a clustering-based nearest neighbor approach such as k-mean clustering. For an item-based method we’ll only consider items that are in the same cluster, thereby reducing the complexity by a factor of k because each item will be compared against only a fraction of the itemset.

\subsection{Clustering based on Computing Embeddings from Metadata}

Instead of computing similarity based on ratings, we can do so based on features extracted from the metadata. We can concatenate the metadata from different analysis – genre (one-hot encoded), sentiment classification from lyrics, and acoustic- and spectral-based based metrics as described earlier. However, we cannot use the vectorized features as-is to compute cosine similarity, because different features may have different meanings. One possibility is to use the vectors but normalize each source. For example, let’s say we have three sources – genre from Tagtraum (1 dimension), lyrics from MusiXmatch (10 dimensions), spectral from MAGD (168 dimensions), we’ll concatenate the normalized features into a vector of 1+10+168 = 179 dimensions, but weighing the dimensions with three hyperparameters that control the importance of individual dimensions.

\subsection{
Neural Ensemble on top of CF or MF recommendations}

We are considering the possibility of analyzing under what circumstances would any one of the techniques out-perform others, to better cater to different scenarios (e.g. recommendation to a user who listens to many obscure songs, vs to a user who listens to a few popular songs). One possibility is to train a neural network, or a gradient-boosting decision tree (GBDT) to classify the user based on its embedding and the number of songs played by the user. We would need to sort out how we should approach this problem, either via hyper-parameter tuning or a simple neural network-based regression or classification. Another possibility is a simple ensemble method aggregating the recommendations from different techniques separately, and make a meta-recommendation prioritized by adding up the normalized score from each recommendation.

\subsection{More Sentiment Analysis on Lyrics and Acoustic and Genre Metadata}

We believe good embedding may hold the key to superior recommendations because of the sparsity of the utility matrix. It may be hard to concoct similarity information from user neighborhoods due to almost non-existent common songs outside of the popular ones. Perhaps it would be more promising to look at songs the user already likes, and find songs that share characteristics with his/her own songs. For example, if a user likes a certain sappy love song about breakup, she may like another sappy love song about breakup. Therefore, we plan to look deeper into sentiment analysis based on lyrics, tempo, and genre information. While we did mention clustering based on computing embeddings from metadata already, we feel this may fall under the ``deeper analysis'' category \cite{chen2018combining}.

\bibliographystyle{acm}
\bibliography{report}

\end{document}
